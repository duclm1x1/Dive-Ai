# Complete RLHF + PPO Training System
## Reinforcement Learning from Human Feedback with Reward Model and Proximal Policy Optimization

**Date:** February 3, 2026  
**Purpose:** Align language models with human preferences using RLHF  
**Techniques:** Reward Model, PPO, Human Feedback Integration

---

## Overview

**RLHF (Reinforcement Learning from Human Feedback)** lÃ  ká»¹ thuáº­t Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n:
- ChatGPT
- Claude
- Gemini
- VÃ  háº§u háº¿t cÃ¡c LLM hiá»‡n Ä‘áº¡i

**3 Stages:**
1. **Supervised Fine-Tuning (SFT)** - Huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao
2. **Reward Model Training** - Há»c tá»« feedback con ngÆ°á»i
3. **PPO Training** - Tá»‘i Æ°u hÃ³a policy dá»±a trÃªn reward

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RLHF Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Stage 1: Human Feedback Collection                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Prompt â†’ Generate 2 Responses â†’ Human Preferenceâ”‚   â”‚
â”‚  â”‚ (A vs B) â†’ Collect Feedback                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â†“                               â”‚
â”‚  Stage 2: Reward Model Training                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Input: (Prompt, Response A, Response B)         â”‚   â”‚
â”‚  â”‚ Target: Preference (A > B, B > A, or Tie)      â”‚   â”‚
â”‚  â”‚ Learn: Bradley-Terry Model                      â”‚   â”‚
â”‚  â”‚ Output: Reward Model r(x, y)                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â†“                               â”‚
â”‚  Stage 3: PPO Policy Training                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Policy: Ï€(y|x)                                  â”‚   â”‚
â”‚  â”‚ Value: V(x)                                     â”‚   â”‚
â”‚  â”‚ Reward: r(x, y) from Reward Model              â”‚   â”‚
â”‚  â”‚ Optimize: PPO Objective                         â”‚   â”‚
â”‚  â”‚ Output: Aligned Policy                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Stage 1: Human Feedback Collection

### Preference Data Format

```python
{
    "prompt": "What is machine learning?",
    "response_a": "Machine learning is a subset of AI.",
    "response_b": "ML learns from data without explicit programming.",
    "preference": 1,  # 0 = A better, 1 = B better, 2 = tie
    "confidence": 0.9,
    "reasoning": "Response B is more comprehensive"
}
```

### Collection Process

1. **Generate Candidates** - Generate 2 responses for each prompt
2. **Human Annotation** - Humans choose which is better
3. **Collect Feedback** - Store preference and confidence
4. **Build Dataset** - Accumulate feedback data

### Example

```python
from rlhf_ppo_system import RLHFPipeline

pipeline = RLHFPipeline()

# Collect feedback
pipeline.stage1_collect_feedback(
    prompt="What is machine learning?",
    response_a="Machine learning is a subset of artificial intelligence.",
    response_b="ML learns from data without being explicitly programmed.",
    preference=1,  # B is better
    confidence=0.9,
    reasoning="Response B is more comprehensive and accurate"
)
```

---

## Stage 2: Reward Model Training

### Reward Model Architecture

```
Input (Prompt + Response)
    â†“
Embedding Layer
    â†“
Transformer Encoder (2-4 layers)
    â†“
Last Token Representation
    â†“
Reward Head (MLP)
    â†“
Output: Scalar Reward r(x, y)
```

### Bradley-Terry Model

The reward model learns to predict preferences using Bradley-Terry model:

```
P(A > B) = sigmoid(r_a - r_b)

Loss = -log(P(preferred > non-preferred))
```

### Training Process

```python
# Train reward model
pipeline.stage2_train_reward_model(
    num_epochs=3,
    batch_size=32
)
```

### Loss Function

```
L_reward = -log(sigmoid(r_preferred - r_non_preferred))
```

### Key Points

- **Preference Learning** - Learn to rank responses
- **Scalar Reward** - Single reward value per response
- **Contrastive Learning** - Compare pairs of responses
- **Generalization** - Learn general preference patterns

---

## Stage 3: PPO Training

### PPO Algorithm

**Proximal Policy Optimization** is a state-of-the-art policy gradient algorithm:

```
Objective:
L^CLIP(Î¸) = E_t[min(r_t(Î¸) Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ) Ã‚_t)]

Where:
- r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_old(a_t|s_t)  (probability ratio)
- Ã‚_t = advantage estimate
- Îµ = clip parameter (typically 0.2)
```

### Policy Network

```
Input (Prompt)
    â†“
Embedding Layer
    â†“
Transformer Encoder
    â†“
Policy Head â†’ Action Logits
    â†“
Value Head â†’ Value Estimate
```

### Advantage Estimation (GAE)

```
Generalized Advantage Estimation:
Ã‚_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)Â²Î´_{t+2} + ...

Where:
Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)  (TD residual)
```

### PPO Loss

```
L_total = L_policy + 0.5 * L_value - Î² * H(Ï€)

Where:
- L_policy = PPO clipped objective
- L_value = MSE(V_predicted, V_target)
- H(Ï€) = entropy bonus (exploration)
```

### Training Process

```python
# Train policy with PPO
pipeline.stage3_train_policy_with_ppo(
    num_epochs=3,
    batch_size=32
)
```

---

## Key Concepts

### 1. Reward Model

**Purpose:** Learn human preferences  
**Input:** (Prompt, Response)  
**Output:** Scalar reward  
**Training:** Preference pairs (A vs B)

```
r_model(prompt, response) â†’ scalar reward
```

### 2. Policy Network

**Purpose:** Generate high-reward responses  
**Input:** Prompt  
**Output:** Response distribution  
**Training:** PPO objective with reward model

```
Ï€(response | prompt) â†’ probability distribution
```

### 3. Value Function

**Purpose:** Estimate expected return  
**Input:** Prompt  
**Output:** Value estimate  
**Training:** Regression on returns

```
V(prompt) â†’ expected return
```

### 4. Advantage Function

**Purpose:** Measure how good an action is relative to average  
**Formula:** A(s,a) = Q(s,a) - V(s)  
**Benefit:** Reduces variance in policy gradient

```
Advantage = Return - Value Estimate
```

---

## Complete Training Loop

### Step 1: Initialize

```python
pipeline = RLHFPipeline(vocab_size=50257)
```

### Step 2: Collect Feedback

```python
for prompt in prompts:
    response_a = generate(prompt, model_a)
    response_b = generate(prompt, model_b)
    
    preference = get_human_preference(response_a, response_b)
    
    pipeline.stage1_collect_feedback(
        prompt, response_a, response_b, preference
    )
```

### Step 3: Train Reward Model

```python
pipeline.stage2_train_reward_model(
    num_epochs=3,
    batch_size=32
)

# Reward model now predicts: r(prompt, response)
```

### Step 4: Generate Rollouts

```python
for prompt in prompts:
    response = policy.generate(prompt)
    reward = reward_model(prompt, response)
    value = value_function(prompt)
    advantage = reward - value
```

### Step 5: Train Policy with PPO

```python
pipeline.stage3_train_policy_with_ppo(
    num_epochs=3,
    batch_size=32
)

# Policy now optimized for high rewards
```

### Step 6: Repeat

Iterate steps 2-5 multiple times for continuous improvement

---

## Hyperparameters

### Reward Model

| Parameter | Value | Description |
|-----------|-------|-------------|
| Learning Rate | 1e-4 | Adam optimizer |
| Batch Size | 32 | Preference pairs |
| Epochs | 3-5 | Training iterations |
| Hidden Size | 768 | Model dimension |
| Num Layers | 2-4 | Transformer layers |

### PPO

| Parameter | Value | Description |
|-----------|-------|-------------|
| Learning Rate | 1e-4 | Adam optimizer |
| Gamma (Î³) | 0.99 | Discount factor |
| GAE Lambda (Î») | 0.95 | Advantage smoothing |
| Clip Ratio (Îµ) | 0.2 | PPO clipping |
| Entropy Coef (Î²) | 0.01 | Exploration bonus |
| Epochs per Batch | 3 | Policy update iterations |

---

## Loss Functions

### Reward Model Loss

```
L_reward = -log(sigmoid(r_preferred - r_non_preferred))

Bradley-Terry model for preference learning
```

### PPO Policy Loss

```
L_policy = E[min(r_t * Ã‚_t, clip(r_t, 1-Îµ, 1+Îµ) * Ã‚_t)]

Clipped objective for stable training
```

### Value Loss

```
L_value = MSE(V_predicted, V_target)

Regression on returns
```

### Entropy Bonus

```
L_entropy = -Î² * H(Ï€) = Î² * Î£ Ï€(a|s) * log(Ï€(a|s))

Encourages exploration
```

### Total Loss

```
L_total = L_policy + 0.5 * L_value - Î² * L_entropy

Balanced training
```

---

## Best Practices

### 1. Data Quality

âœ… **Do:**
- Collect diverse feedback
- Use confident annotators
- Ensure consistent labeling
- Validate data quality

âŒ **Don't:**
- Use low-quality feedback
- Mix different annotation styles
- Include biased preferences
- Ignore annotation disagreement

### 2. Reward Model Training

âœ… **Do:**
- Train on diverse preferences
- Monitor loss curves
- Validate on held-out data
- Check reward model calibration

âŒ **Don't:**
- Overfit to small dataset
- Ignore preference distribution
- Train without validation
- Use uncalibrated rewards

### 3. PPO Training

âœ… **Do:**
- Use appropriate learning rates
- Monitor policy divergence
- Check advantage estimates
- Validate on test prompts

âŒ **Don't:**
- Use too high learning rates
- Train for too many epochs
- Ignore KL divergence
- Overfit to reward model

### 4. Monitoring

âœ… **Do:**
- Track reward model accuracy
- Monitor policy performance
- Check KL divergence from base model
- Validate on human evaluation

âŒ **Don't:**
- Ignore training metrics
- Train blindly
- Skip validation
- Assume convergence

---

## Troubleshooting

### Problem: Reward Model Not Learning

**Causes:**
- Insufficient feedback data
- Low quality annotations
- Imbalanced preferences
- Poor hyperparameters

**Solutions:**
- Collect more feedback
- Improve annotation quality
- Balance preference distribution
- Tune learning rate

### Problem: Policy Diverging from Base Model

**Causes:**
- Too high learning rate
- Insufficient KL penalty
- Reward model overfitting
- Too many PPO epochs

**Solutions:**
- Lower learning rate
- Increase KL penalty
- Validate reward model
- Reduce PPO epochs

### Problem: Reward Hacking

**Causes:**
- Reward model exploiting edge cases
- Unrealistic responses
- Gaming the reward signal

**Solutions:**
- Add KL penalty to objective
- Monitor response quality
- Use human evaluation
- Diversify reward model training

---

## Evaluation

### Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| **Reward Model Accuracy** | Preference prediction accuracy | >80% |
| **Policy Reward** | Average reward on test set | High |
| **KL Divergence** | Distance from base model | Low |
| **Human Evaluation** | Human preference for aligned model | >70% |
| **Response Quality** | Fluency, coherence, correctness | High |

### Validation

```python
# Get training status
status = pipeline.get_training_status()

print(f"Feedback Statistics: {status['feedback_statistics']}")
print(f"Reward Model Metrics: {status['reward_model_metrics']}")
print(f"PPO Metrics: {status['ppo_metrics']}")
```

---

## References

### Papers

1. **RLHF Origins**
   - Christiano et al. (2017) - "Deep Reinforcement Learning from Human Preferences"

2. **PPO Algorithm**
   - Schulman et al. (2017) - "Proximal Policy Optimization Algorithms"

3. **Reward Model**
   - Ziegler et al. (2019) - "Fine-Tuning Language Models from Human Preferences"

4. **InstructGPT/ChatGPT**
   - Ouyang et al. (2022) - "Training language models to follow instructions with human feedback"

### Key Concepts

- **Bradley-Terry Model** - Preference learning
- **Generalized Advantage Estimation (GAE)** - Advantage estimation
- **Proximal Policy Optimization (PPO)** - Policy optimization
- **KL Divergence** - Divergence penalty
- **Entropy Bonus** - Exploration encouragement

---

## Conclusion

**RLHF + PPO is the state-of-the-art approach for aligning language models with human preferences.**

**Key Steps:**
1. Collect human feedback on response pairs
2. Train reward model to predict preferences
3. Use PPO to optimize policy based on rewards
4. Iterate for continuous improvement

**Result:** Language models that are helpful, harmless, and honest!

---

**System ready for production RLHF training!** ðŸš€
