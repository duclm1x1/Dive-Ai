# Advanced Reasoning Model - Complete Guide
## Chain-of-Thought with Intermediate Step Verification and Self-Correction

**Date:** February 3, 2026  
**Purpose:** Build models that reason step-by-step like humans  
**Techniques:** SFT + Reward Model + PPO + Chain-of-Thought

---

## Overview

**Reasoning Model** lÃ  mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng:

1. **Suy luáº­n tá»«ng bÆ°á»›c** - Chain-of-Thought (CoT)
2. **Kiá»ƒm tra trung gian** - Verify intermediate steps
3. **Tá»± sá»­a lá»—i** - Self-correction
4. **Giáº£i thÃ­ch rÃµ rÃ ng** - Explainability

**á»¨ng dá»¥ng:**
- Giáº£i toÃ¡n phá»©c táº¡p
- PhÃ¢n tÃ­ch logic
- Giáº£i quyáº¿t váº¥n Ä‘á»
- Suy luáº­n khoa há»c

---

## 3 Stages Training

### Stage 1: Supervised Fine-Tuning (SFT)

**Má»¥c Ä‘Ã­ch:** Dáº¡y mÃ´ hÃ¬nh cÃ¡ch suy luáº­n tá»«ng bÆ°á»›c

**Dá»¯ liá»‡u:**
- Demonstration data vá»›i reasoning traces
- Má»—i trace cÃ³ nhiá»u steps
- Má»—i step lÃ  má»™t suy luáº­n

**VÃ­ dá»¥:**

```
Problem: What is 1 + 2 + 3 + ... + 100?

Step 1: Analyze the problem
The problem asks us to find the sum of all numbers from 1 to 100.

Step 2: Recall the formula
I remember that the sum of first n natural numbers is n(n+1)/2.

Step 3: Apply the formula
For n = 100, the sum = 100 * 101 / 2 = 10100 / 2 = 5050.

Step 4: Verify the result
Let me verify: 1 + 2 + ... + 100 should equal 5050.
This seems reasonable because the average is 50.5 and 50.5 * 100 = 5050.

Final Answer: 5050
```

**Metrics:**
- Average reasoning steps per example
- Average reasoning quality
- Coverage of reasoning types

---

### Stage 2: Reward Model Training

**Má»¥c Ä‘Ã­ch:** Há»c Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng suy luáº­n

**Dá»¯ liá»‡u:**
- Comparison data (A vs B vs C vs D)
- Ranking tá»« tá»‘t nháº¥t Ä‘áº¿n tá»‡ nháº¥t
- Pháº£n há»“i con ngÆ°á»i

**VÃ­ dá»¥:**

```
Problem: Explain gravity

Response A: "Gravity is a force that attracts objects."
Response B: "Gravity is a fundamental force that attracts all objects with mass. 
            The force is proportional to mass and inversely proportional to distance squared."
Response C: "Gravity exists."
Response D: "Gravity pulls things down."

Ranking: B > A > D > C
```

**Reward Model há»c:**
- ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng suy luáº­n
- Kiá»ƒm tra tÃ­nh logic
- XÃ¡c minh tÃ­nh chÃ­nh xÃ¡c
- ÄÃ¡nh giÃ¡ tÃ­nh hoÃ n chá»‰nh

**Metrics:**
- Preference prediction accuracy
- Verification quality scores
- Ranking correlation

---

### Stage 3: RL with PPO

**Má»¥c Ä‘Ã­ch:** Tá»‘i Æ°u hÃ³a policy dá»±a trÃªn rewards

**QuÃ¡ trÃ¬nh:**
1. Generate reasoning traces
2. Get rewards from reward model
3. Update policy using PPO
4. Repeat

**Metrics:**
- Average reward
- Policy improvement
- KL divergence from base model

---

## Chain-of-Thought Framework

### Reasoning Types

**8 loáº¡i suy luáº­n:**

| Type | Description | Example |
|------|-------------|---------|
| **Analysis** | Breaking down problem | "Let me analyze this..." |
| **Deduction** | Drawing conclusions | "Therefore, we can conclude..." |
| **Verification** | Checking results | "Let me verify this..." |
| **Refinement** | Improving steps | "Actually, let me reconsider..." |
| **Synthesis** | Combining information | "Combining these facts..." |
| **Elimination** | Ruling out options | "This can't be right because..." |
| **Analogy** | Using similar cases | "This is similar to..." |
| **Calculation** | Math operations | "2 + 3 = 5" |

### CoT Prompt Engineering

**Basic CoT:**
```
Q: What is 2 + 3?
A: Let me think step by step.
2 + 3 = 5.
```

**Better CoT:**
```
Q: What is 2 + 3?
A: Let me think step by step.
Step 1: I have 2 objects.
Step 2: I add 3 more objects.
Step 3: Counting all objects: 1, 2, 3, 4, 5.
Therefore, 2 + 3 = 5.
```

**Best CoT:**
```
Q: What is 2 + 3?
A: Let me solve this step by step.

Step 1: Understand the problem
I need to add two numbers: 2 and 3.

Step 2: Apply addition
2 + 3 means combining 2 and 3.

Step 3: Count the result
Starting from 2: 2 â†’ 3 â†’ 4 â†’ 5
So the result is 5.

Step 4: Verify
Check: 3 + 2 = 5 (commutative property holds)
The answer is correct.

Final Answer: 5
```

---

## Intermediate Step Verification

### Verification Rules

**4 verification dimensions:**

| Rule | Weight | Description |
|------|--------|-------------|
| **Logical Consistency** | 30% | Does step follow from previous? |
| **Mathematical Correctness** | 25% | Are calculations correct? |
| **Factual Accuracy** | 25% | Are facts accurate? |
| **Completeness** | 20% | Does step address problem? |

### Verification Process

```
For each step:
1. Check logical consistency with previous steps
2. Check mathematical correctness
3. Check factual accuracy
4. Check completeness
5. Calculate overall verification score
```

### Example Verification

```
Step: "2 + 3 = 5"

Logical Consistency: 0.9 (follows from problem)
Mathematical Correctness: 1.0 (correct)
Factual Accuracy: 1.0 (accurate)
Completeness: 0.8 (addresses problem)

Overall Score: 0.9*0.3 + 1.0*0.25 + 1.0*0.25 + 0.8*0.2 = 0.94
```

---

## Self-Correction Mechanism

### Error Detection

**Identify errors by:**
1. Verify each step
2. Find steps with low verification scores
3. Identify specific issues

**Issues:**
- Low logical consistency
- Mathematical errors
- Factual inaccuracies
- Incomplete steps

### Correction Process

```
For each error:
1. Identify the problematic step
2. Generate correction prompt
3. Model generates corrected step
4. Verify corrected step
5. Repeat until no errors or max attempts
```

### Example Correction

**Original (Wrong):**
```
Step 1: 2 + 3 = 6
```

**Correction Prompt:**
```
The reasoning at Step 1 may have issues:
Current: 2 + 3 = 6
Issues: Mathematical correctness is low

Please reconsider this step:
Step 1 (Corrected):
```

**Corrected:**
```
Step 1: 2 + 3 = 5 (correct arithmetic)
```

---

## Reasoning-Specific Reward Model

### Reward Components

**Three reward signals:**

1. **Step Quality Reward** (30%)
   - Quality of individual steps
   - Clarity and correctness

2. **Trace Coherence Reward** (40%)
   - How well steps connect
   - Logical flow

3. **Correctness Reward** (30%)
   - Final answer correctness
   - Overall solution quality

### Reward Calculation

```
Total Reward = 0.3 * step_quality + 0.4 * trace_coherence + 0.3 * correctness

Where:
- step_quality: average quality of all steps
- trace_coherence: how well steps connect
- correctness: whether final answer is correct
```

### Example Rewards

```
Good Reasoning:
- Step Quality: 0.9
- Trace Coherence: 0.95
- Correctness: 1.0
- Total: 0.3*0.9 + 0.4*0.95 + 0.3*1.0 = 0.95

Poor Reasoning:
- Step Quality: 0.5
- Trace Coherence: 0.4
- Correctness: 0.0
- Total: 0.3*0.5 + 0.4*0.4 + 0.3*0.0 = 0.31
```

---

## Complete Training Pipeline

### Step 1: Collect Demonstration Data

```python
demonstration_data = [
    {
        "problem": "What is 1 + 2 + ... + 100?",
        "reasoning": """
        Step 1: Use formula for sum of n natural numbers
        Step 2: Apply n(n+1)/2 with n=100
        Step 3: Calculate 100*101/2 = 5050
        Final Answer: 5050
        """
    },
    # ... more examples
]
```

### Step 2: Stage 1 - SFT Training

```python
trainer = ReasoningModelTrainer()

sft_metrics = trainer.stage1_sft_training(demonstration_data)
# Output: avg_reasoning_steps, avg_reasoning_quality
```

### Step 3: Collect Comparison Data

```python
comparison_data = [
    {
        "trace_a": "...",  # reasoning trace A
        "trace_b": "...",  # reasoning trace B
        "preference": 0    # 0 = A better, 1 = B better
    },
    # ... more comparisons
]
```

### Step 4: Stage 2 - Reward Model Training

```python
reward_metrics = trainer.stage2_reward_model_training(comparison_data)
# Output: avg_verification_score
```

### Step 5: Stage 3 - RL Training

```python
rl_metrics = trainer.stage3_rl_training(num_iterations=10)
# Output: avg_reward, avg_reasoning_quality
```

---

## Usage Examples

### Example 1: Parse Reasoning Trace

```python
from advanced_reasoning_model import ChainOfThoughtGenerator

generator = ChainOfThoughtGenerator()

reasoning_text = """
Step 1: Analyze the problem
...

Step 2: Apply the formula
...

Final Answer: 5050
"""

trace = generator.parse_reasoning_trace(reasoning_text)

print(f"Number of steps: {len(trace.steps)}")
print(f"Reasoning quality: {trace.reasoning_quality:.1%}")

for step in trace.steps:
    print(f"  Step {step.step_number}: {step.reasoning_type}")
```

### Example 2: Verify Reasoning

```python
from advanced_reasoning_model import IntermediateStepVerifier

verifier = IntermediateStepVerifier()

verification = verifier.verify_trace(trace)

print(f"Overall quality: {verification['overall_quality']:.1%}")

for i, step_verification in enumerate(verification['steps_verification']):
    print(f"  Step {i}: {step_verification['overall_score']:.1%}")
```

### Example 3: Self-Correction

```python
from advanced_reasoning_model import SelfCorrectionMechanism

corrector = SelfCorrectionMechanism()

errors = corrector.identify_errors(trace)

if errors:
    print(f"Found {len(errors)} errors:")
    for error in errors:
        print(f"  Step {error['step_number']}: {error['issues']}")
    
    # Generate correction prompts
    for error in errors:
        prompt = corrector.generate_correction_prompt(trace, error)
        print(f"\nCorrection prompt:\n{prompt}")
```

---

## Performance Metrics

### Reasoning Quality Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| **Step Accuracy** | Correctness of individual steps | >85% |
| **Trace Coherence** | Logical flow between steps | >80% |
| **Final Accuracy** | Correctness of final answer | >90% |
| **Explanation Quality** | Clarity and completeness | >80% |

### Training Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| **SFT Loss** | Supervised fine-tuning loss | <0.5 |
| **Reward Model Accuracy** | Preference prediction | >75% |
| **PPO Reward** | Average reward from RL | >0.8 |
| **KL Divergence** | Distance from base model | <1.0 |

---

## Best Practices

### 1. Data Collection

âœ… **Do:**
- Collect diverse reasoning traces
- Include multiple reasoning types
- Ensure high-quality demonstrations
- Validate reasoning correctness

âŒ **Don't:**
- Use low-quality reasoning
- Include circular logic
- Mix different domains
- Ignore reasoning quality

### 2. Training

âœ… **Do:**
- Start with SFT on good examples
- Verify reward model accuracy
- Monitor training metrics
- Validate on test set

âŒ **Don't:**
- Skip SFT stage
- Use uncalibrated rewards
- Train without validation
- Overfit to small dataset

### 3. Evaluation

âœ… **Do:**
- Test on diverse problems
- Evaluate reasoning quality
- Check step-by-step correctness
- Validate with humans

âŒ **Don't:**
- Only check final answer
- Ignore reasoning quality
- Skip human evaluation
- Assume generalization

---

## Troubleshooting

### Problem: Model Generates Incoherent Steps

**Causes:**
- Poor SFT data quality
- Insufficient training
- Weak reward model

**Solutions:**
- Improve demonstration data
- Increase training iterations
- Validate reward model

### Problem: Steps Are Correct But Trace Is Incoherent

**Causes:**
- Weak trace coherence reward
- Poor step connections
- Missing context

**Solutions:**
- Increase trace coherence weight
- Add connection verification
- Improve context passing

### Problem: Model Doesn't Self-Correct

**Causes:**
- Weak error detection
- Poor correction prompts
- Limited correction attempts

**Solutions:**
- Improve verification rules
- Better correction prompts
- Increase max attempts

---

## Conclusion

**Advanced Reasoning Model enables:**

âœ… Step-by-step reasoning  
âœ… Intermediate verification  
âœ… Self-correction  
âœ… Explainability  
âœ… Better problem-solving  

**Key Stages:**
1. SFT - Learn to reason
2. Reward Model - Learn to evaluate
3. PPO - Learn to optimize

**Result:** Models that reason like humans! ðŸ§ 

---

**System ready for advanced reasoning training!** ðŸš€
