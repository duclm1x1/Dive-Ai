#!/usr/bin/env python3\n\"\"\"\nTransmission Optimizer - T·ªëi ∆∞u h√≥a ƒë∆∞·ªùng truy·ªÅn d·ªØ li·ªáu\nCompression, Batching, Caching, Deduplication\n\"\"\"\n\nimport json\nimport logging\nimport gzip\nimport zlib\nimport hashlib\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime, timedelta\nfrom collections import deque\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass CompressionStrategy:\n    \"\"\"Compression strategy\"\"\"\n    \n    @staticmethod\n    def gzip_compress(data: str, level: int = 6) -> bytes:\n        \"\"\"Compress with gzip\"\"\"\n        return gzip.compress(data.encode('utf-8'), compresslevel=level)\n    \n    @staticmethod\n    def gzip_decompress(data: bytes) -> str:\n        \"\"\"Decompress gzip\"\"\"\n        return gzip.decompress(data).decode('utf-8')\n    \n    @staticmethod\n    def deflate_compress(data: str, level: int = 6) -> bytes:\n        \"\"\"Compress with deflate\"\"\"\n        return zlib.compress(data.encode('utf-8'), level=level)\n    \n    @staticmethod\n    def deflate_decompress(data: bytes) -> str:\n        \"\"\"Decompress deflate\"\"\"\n        return zlib.decompress(data).decode('utf-8')\n\nclass BatchingStrategy:\n    \"\"\"Batching strategy\"\"\"\n    \n    def __init__(self, batch_size: int = 100, batch_timeout: int = 5):\n        self.batch_size = batch_size\n        self.batch_timeout = batch_timeout\n        self.batch: List[Any] = []\n        self.batch_created_at = datetime.now()\n    \n    def add_item(self, item: Any) -> Optional[List[Any]]:\n        \"\"\"Add item to batch\"\"\"\n        self.batch.append(item)\n        \n        # Check if batch is full\n        if len(self.batch) >= self.batch_size:\n            return self.get_batch()\n        \n        # Check if batch has timed out\n        if (datetime.now() - self.batch_created_at).total_seconds() >= self.batch_timeout:\n            if self.batch:\n                return self.get_batch()\n        \n        return None\n    \n    def get_batch(self) -> Optional[List[Any]]:\n        \"\"\"Get current batch\"\"\"\n        if not self.batch:\n            return None\n        \n        batch = self.batch.copy()\n        self.batch = []\n        self.batch_created_at = datetime.now()\n        \n        return batch\n\nclass CacheStrategy:\n    \"\"\"Cache strategy\"\"\"\n    \n    def __init__(self, max_size: int = 1000, ttl: int = 3600):\n        self.max_size = max_size\n        self.ttl = ttl\n        self.cache: Dict[str, Dict[str, Any]] = {}\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"Get from cache\"\"\"\n        if key not in self.cache:\n            return None\n        \n        entry = self.cache[key]\n        \n        # Check if expired\n        if (datetime.now() - entry[\"created_at\"]).total_seconds() > self.ttl:\n            del self.cache[key]\n            return None\n        \n        entry[\"hits\"] += 1\n        return entry[\"value\"]\n    \n    def set(self, key: str, value: Any):\n        \"\"\"Set in cache\"\"\"\n        # Evict oldest entry if cache is full\n        if len(self.cache) >= self.max_size:\n            oldest_key = min(self.cache.keys(), \n                            key=lambda k: self.cache[k][\"created_at\"])\n            del self.cache[oldest_key]\n        \n        self.cache[key] = {\n            \"value\": value,\n            \"created_at\": datetime.now(),\n            \"hits\": 0\n        }\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics\"\"\"\n        total_hits = sum(entry[\"hits\"] for entry in self.cache.values())\n        \n        return {\n            \"size\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"utilization\": f\"{(len(self.cache) / self.max_size * 100):.1f}%\",\n            \"total_hits\": total_hits,\n            \"avg_hits\": f\"{(total_hits / len(self.cache)):.1f}\" if self.cache else \"0\"\n        }\n\nclass DeduplicationStrategy:\n    \"\"\"Deduplication strategy\"\"\"\n    \n    def __init__(self):\n        self.seen_hashes: set = set()\n        self.dedup_count = 0\n    \n    def is_duplicate(self, data: Any) -> bool:\n        \"\"\"Check if data is duplicate\"\"\"\n        data_str = json.dumps(data, sort_keys=True, default=str)\n        data_hash = hashlib.md5(data_str.encode()).hexdigest()\n        \n        if data_hash in self.seen_hashes:\n            self.dedup_count += 1\n            return True\n        \n        self.seen_hashes.add(data_hash)\n        return False\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get deduplication statistics\"\"\"\n        return {\n            \"total_items_seen\": len(self.seen_hashes) + self.dedup_count,\n            \"unique_items\": len(self.seen_hashes),\n            \"duplicates_removed\": self.dedup_count,\n            \"dedup_rate\": f\"{(self.dedup_count / (len(self.seen_hashes) + self.dedup_count) * 100):.1f}%\" if (len(self.seen_hashes) + self.dedup_count) > 0 else \"0%\"\n        }\n\nclass TransmissionOptimizer:\n    \"\"\"Transmission Optimizer - T·ªëi ∆∞u h√≥a ƒë∆∞·ªùng truy·ªÅn\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize Transmission Optimizer\"\"\"\n        self.compression_enabled = False\n        self.compression_type = None\n        self.compression_level = 6\n        \n        self.batching_enabled = False\n        self.batching_strategy = None\n        \n        self.caching_enabled = False\n        self.cache_strategy = None\n        \n        self.deduplication_enabled = False\n        self.dedup_strategy = DeduplicationStrategy()\n        \n        self.stats = {\n            \"original_size\": 0,\n            \"optimized_size\": 0,\n            \"compression_ratio\": 0,\n            \"items_processed\": 0\n        }\n        \n        logger.info(\"‚úÖ Transmission Optimizer initialized\")\n    \n    def enable_compression(self, compression_type: str = \"gzip\", level: int = 6):\n        \"\"\"Enable compression\"\"\"\n        self.compression_enabled = True\n        self.compression_type = compression_type\n        self.compression_level = level\n        logger.info(f\"‚úÖ Compression enabled ({compression_type}, level={level})\")\n    \n    def enable_batching(self, batch_size: int = 100, batch_timeout: int = 5):\n        \"\"\"Enable batching\"\"\"\n        self.batching_enabled = True\n        self.batching_strategy = BatchingStrategy(batch_size, batch_timeout)\n        logger.info(f\"‚úÖ Batching enabled (size={batch_size}, timeout={batch_timeout}s)\")\n    \n    def enable_caching(self, max_size: int = 1000, ttl: int = 3600):\n        \"\"\"Enable caching\"\"\"\n        self.caching_enabled = True\n        self.cache_strategy = CacheStrategy(max_size, ttl)\n        logger.info(f\"‚úÖ Caching enabled (max_size={max_size}, ttl={ttl}s)\")\n    \n    def enable_deduplication(self):\n        \"\"\"Enable deduplication\"\"\"\n        self.deduplication_enabled = True\n        logger.info(\"‚úÖ Deduplication enabled\")\n    \n    def optimize(self, data: Any) -> bytes:\n        \"\"\"Optimize data for transmission\"\"\"\n        \n        # Convert to JSON string\n        if not isinstance(data, str):\n            data_str = json.dumps(data, default=str)\n        else:\n            data_str = data\n        \n        original_size = len(data_str.encode('utf-8'))\n        self.stats[\"original_size\"] += original_size\n        self.stats[\"items_processed\"] += 1\n        \n        # Check cache\n        if self.caching_enabled and self.cache_strategy:\n            cache_key = hashlib.md5(data_str.encode()).hexdigest()\n            cached = self.cache_strategy.get(cache_key)\n            if cached is not None:\n                logger.info(f\"‚úÖ Cache hit (saved {original_size} bytes)\")\n                return cached\n        \n        # Check deduplication\n        if self.deduplication_enabled and self.dedup_strategy.is_duplicate(data):\n            logger.info(\"‚úÖ Duplicate detected (skipped)\")\n            return b\"\"\n        \n        # Compress\n        if self.compression_enabled:\n            if self.compression_type == \"gzip\":\n                optimized = CompressionStrategy.gzip_compress(data_str, self.compression_level)\n            elif self.compression_type == \"deflate\":\n                optimized = CompressionStrategy.deflate_compress(data_str, self.compression_level)\n            else:\n                optimized = data_str.encode('utf-8')\n        else:\n            optimized = data_str.encode('utf-8')\n        \n        optimized_size = len(optimized)\n        self.stats[\"optimized_size\"] += optimized_size\n        \n        # Calculate compression ratio\n        if original_size > 0:\n            ratio = (1 - optimized_size / original_size) * 100\n            self.stats[\"compression_ratio\"] = ratio\n            logger.info(f\"‚úÖ Optimized: {original_size} ‚Üí {optimized_size} bytes ({ratio:.1f}% reduction)\")\n        \n        # Cache optimized data\n        if self.caching_enabled and self.cache_strategy:\n            cache_key = hashlib.md5(data_str.encode()).hexdigest()\n            self.cache_strategy.set(cache_key, optimized)\n        \n        return optimized\n    \n    def get_optimization_stats(self) -> Dict[str, Any]:\n        \"\"\"Get optimization statistics\"\"\"\n        stats = {\n            \"compression\": {\n                \"enabled\": self.compression_enabled,\n                \"type\": self.compression_type,\n                \"level\": self.compression_level\n            },\n            \"batching\": {\n                \"enabled\": self.batching_enabled\n            },\n            \"caching\": {\n                \"enabled\": self.caching_enabled\n            },\n            \"deduplication\": {\n                \"enabled\": self.deduplication_enabled\n            },\n            \"transmission_stats\": {\n                \"original_size\": self.stats[\"original_size\"],\n                \"optimized_size\": self.stats[\"optimized_size\"],\n                \"compression_ratio\": f\"{self.stats['compression_ratio']:.1f}%\",\n                \"items_processed\": self.stats[\"items_processed\"]\n            }\n        }\n        \n        if self.caching_enabled and self.cache_strategy:\n            stats[\"cache_stats\"] = self.cache_strategy.get_stats()\n        \n        if self.deduplication_enabled:\n            stats[\"dedup_stats\"] = self.dedup_strategy.get_stats()\n        \n        return stats\n    \n    def print_optimization_info(self):\n        \"\"\"Print optimization information\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"TRANSMISSION OPTIMIZER - INFORMATION\")\n        print(\"=\"*80)\n        \n        stats = self.get_optimization_stats()\n        \n        print(f\"\\nüîß Optimization Strategies:\")\n        print(f\"  Compression: {'‚úÖ Enabled' if stats['compression']['enabled'] else '‚ùå Disabled'} ({stats['compression']['type']})\")\n        print(f\"  Batching: {'‚úÖ Enabled' if stats['batching']['enabled'] else '‚ùå Disabled'}\")\n        print(f\"  Caching: {'‚úÖ Enabled' if stats['caching']['enabled'] else '‚ùå Disabled'}\")\n        print(f\"  Deduplication: {'‚úÖ Enabled' if stats['deduplication']['enabled'] else '‚ùå Disabled'}\")\n        \n        print(f\"\\nüìä Transmission Statistics:\")\n        trans = stats[\"transmission_stats\"]\n        print(f\"  Original Size: {trans['original_size']} bytes\")\n        print(f\"  Optimized Size: {trans['optimized_size']} bytes\")\n        print(f\"  Compression Ratio: {trans['compression_ratio']}\")\n        print(f\"  Items Processed: {trans['items_processed']}\")\n        \n        if \"cache_stats\" in stats:\n            print(f\"\\nüíæ Cache Statistics:\")\n            cache = stats[\"cache_stats\"]\n            print(f\"  Size: {cache['size']}/{cache['max_size']}\")\n            print(f\"  Utilization: {cache['utilization']}\")\n            print(f\"  Total Hits: {cache['total_hits']}\")\n        \n        if \"dedup_stats\" in stats:\n            print(f\"\\nüîÑ Deduplication Statistics:\")\n            dedup = stats[\"dedup_stats\"]\n            print(f\"  Total Items: {dedup['total_items_seen']}\")\n            print(f\"  Unique Items: {dedup['unique_items']}\")\n            print(f\"  Duplicates Removed: {dedup['duplicates_removed']}\")\n            print(f\"  Dedup Rate: {dedup['dedup_rate']}\")\n        \n        print(\"=\"*80)\n\ndef main():\n    \"\"\"Test Transmission Optimizer\"\"\"\n    \n    optimizer = TransmissionOptimizer()\n    \n    # Enable all optimization strategies\n    optimizer.enable_compression(\"gzip\", level=6)\n    optimizer.enable_batching(batch_size=100)\n    optimizer.enable_caching(max_size=1000, ttl=3600)\n    optimizer.enable_deduplication()\n    \n    print(\"\\nüîÑ Testing Transmission Optimizer...\\n\")\n    \n    # Create test data\n    test_data = [\n        {\"id\": i, \"name\": f\"Item {i}\", \"value\": f\"Value {i}\" * 10}\n        for i in range(100)\n    ]\n    \n    # Optimize data\n    optimized = optimizer.optimize(test_data)\n    print(f\"\\nOptimized data size: {len(optimized)} bytes\")\n    \n    # Optimize same data again (should hit cache)\n    optimized2 = optimizer.optimize(test_data)\n    \n    # Print optimization info\n    optimizer.print_optimization_info()\n\nif __name__ == \"__main__\":\n    main()\n
